{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "521296f6",
   "metadata": {},
   "source": [
    "#### You can use Parquet, a columnar storage file format that is optimized for use with big data processing frameworks. Here’s how you can modify your function to first write the DataFrame to a Parquet file, and then load the Parquet file into MySQL:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ec5d1a",
   "metadata": {},
   "source": [
    "#### **Parquet** and **PyArrow** are two different but related technologies:\n",
    "\n",
    "##### **Parquet** is an efficient, compressed, column-oriented storage format for arrays and tables of data³. It's designed to bring efficient columnar storage of data closer to the processing, to make the processing faster. It's often used in big data processing frameworks³.\n",
    "\n",
    "##### **PyArrow**, on the other hand, is a Python library that is part of the Apache Arrow project³. It provides Python APIs to use and manipulate Arrow's in-memory columnar format³. PyArrow also includes a Pythonic API for reading and writing Parquet files³.\n",
    "\n",
    "##### So, in essence, Parquet is a storage format, and PyArrow is a Python library that can read and write this format, among other things³. They are often used together in data processing pipelines, especially in big data contexts³.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d716659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mysqlclient in c:\\users\\bookie\\anaconda3\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\bookie\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: pymysql in c:\\users\\bookie\\anaconda3\\lib\\site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install mysqlclient\n",
    "!pip install python-dotenv\n",
    "!pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a808ce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import logging\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "# Access environment variables\n",
    "load_dotenv(override=True, encoding='utf-16')\n",
    "\n",
    "# Access environment variables\n",
    "database_url = os.getenv('DATABASE_URL')\n",
    "\n",
    "def load_csv_to_mysql_parquet(file_path, table_name, database_url):\n",
    "    try: \n",
    "        # Create a connection to the MySQL database using the provided database URL\n",
    "        engine = create_engine(database_url, pool_size=10, max_overflow=20)\n",
    "\n",
    "        # Read the data from the csv file into a pandas dataframe\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Convert the pandas dataframe to a PyArrow Table\n",
    "        table = pa.Table.from_pandas(df)\n",
    "\n",
    "        # Write the PyArrow Table to a Parquet file\n",
    "        pq.write_table(table, 'temp.parquet')\n",
    "\n",
    "        # Read the Parquet file into a new pandas dataframe\n",
    "        df_new = pd.read_parquet('temp.parquet')\n",
    "\n",
    "        # Use pandas to_sql function to load data from DataFrame into MySQL table\n",
    "        df_new.to_sql(table_name, con=engine, if_exists='append', chunksize=100000, index=False)\n",
    "\n",
    "        print(f\"Data loaded successfully into the '{table_name}' table.\" )\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"Error: CSV file '{file_path}' not found. {e}\")\n",
    "    \n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        logging.error(f\"Error: CSV file '{file_path}' is empty. {e}\")\n",
    "        \n",
    "    except SQLAlchemyError as e:\n",
    "        logging.error(f\"SQLAlchemy Error: {e}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        if 'engine' in locals():\n",
    "            engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed5d5c9",
   "metadata": {},
   "source": [
    "##### In this modified version of your function, pd.read_csv(file_path) returns a pandas DataFrame. The DataFrame is then converted to a PyArrow Table and written to a Parquet file. The Parquet file is read back into a new pandas DataFrame, which is then written to the MySQL table using the to_sql method.\n",
    "\n",
    "##### Please note that this approach involves writing and reading a temporary Parquet file (‘temp.parquet’), so make sure you have enough disk space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16988a98",
   "metadata": {},
   "source": [
    "#### Load users data into the Users Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f09f6fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully into the 'users' table.\n"
     ]
    }
   ],
   "source": [
    "#Usage: Calling the function\n",
    "file_path = \"C:/Users/Bookie/Documents/Dufuna_Documentation/CapstoneProject/data_tables/users.csv\"\n",
    "table_name = \"users\"\n",
    "database_url = os.getenv('DATABASE_URL')\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    load_csv_to_mysql_parquet(file_path, table_name, database_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54ade08",
   "metadata": {},
   "source": [
    "**Load users data into the Events Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e102e732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully into the 'events' table.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"C:/Users/Bookie/Documents/Dufuna_Documentation/CapstoneProject/data_tables/events.csv\"\n",
    "table_name = \"events\"\n",
    "database_url = os.getenv('DATABASE_URL')\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "        load_csv_to_mysql_parquet(file_path, table_name, database_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94118bd1",
   "metadata": {},
   "source": [
    "**Load users data into the Distribution_Centers Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e6440ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully into the 'distribution_centers' table.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"C:/Users/Bookie/Documents/Dufuna_Documentation/CapstoneProject/data_tables/distribution_centers.csv\"\n",
    "table_name = \"distribution_centers\"\n",
    "database_url = os.getenv('DATABASE_URL')\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    load_csv_to_mysql_parquet(file_path, table_name, database_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c62bf8",
   "metadata": {},
   "source": [
    "**Load users data into the Orders Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6d99b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully into the 'orders' table.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"C:/Users/Bookie/Documents/Dufuna_Documentation/CapstoneProject/data_tables/orders.csv\"\n",
    "table_name = \"orders\"\n",
    "database_url = os.getenv('DATABASE_URL')\n",
    "\n",
    "if __name__==\"__main__\":    \n",
    "    load_csv_to_mysql_parquet(file_path, table_name, database_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb0f64d",
   "metadata": {},
   "source": [
    "**Load users data into the Products Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "000662aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully into the 'products' table.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"C:/Users/Bookie/Documents/Dufuna_Documentation/CapstoneProject/data_tables/products.csv\"\n",
    "table_name = \"products\"\n",
    "database_url = os.getenv('DATABASE_URL')\n",
    "\n",
    "if __name__==\"__main__\":    \n",
    "    load_csv_to_mysql_parquet(file_path, table_name, database_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad6a959",
   "metadata": {},
   "source": [
    "**Load users data into the Inventory_Items Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "278b871b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully into the 'inventory_items' table.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"C:/Users/Bookie/Documents/Dufuna_Documentation/CapstoneProject/data_tables/inventory_items.csv\"\n",
    "table_name = \"inventory_items\"\n",
    "database_url = os.getenv('DATABASE_URL')\n",
    "\n",
    "if __name__==\"__main__\":    \n",
    "    load_csv_to_mysql_parquet(file_path, table_name, database_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4b49a",
   "metadata": {},
   "source": [
    "**Load users data into the Order_Items Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ec7fd595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully into the 'order_items' table.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"C:/Users/Bookie/Documents/Dufuna_Documentation/CapstoneProject/data_tables/order_items.csv\"\n",
    "table_name = \"order_items\"\n",
    "database_url = os.getenv('DATABASE_URL')\n",
    "\n",
    "if __name__==\"__main__\":    \n",
    "    load_csv_to_mysql_parquet(file_path, table_name, database_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ece92b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
