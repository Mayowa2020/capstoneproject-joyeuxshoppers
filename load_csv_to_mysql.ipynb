{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "397adfde",
   "metadata": {},
   "source": [
    "Overall, this code defines a function to load data from a CSV file into a MySQL database table using PyArrow for data conversion and Parquet file format for intermediate storage. It also includes error handling to manage potential issues during the data loading process.\n",
    "\n",
    "I'll explain the provided code line by line:\n",
    "\n",
    "1. `import pandas as pd`: Imports the pandas library and aliases it as `pd` for easier use in the code.\n",
    "\n",
    "2. `from sqlalchemy import create_engine`: Imports the `create_engine` function from the SQLAlchemy library, which is used to create a connection to a SQL database.\n",
    "\n",
    "3. `from dotenv import load_dotenv`: Imports the `load_dotenv` function from the `dotenv` library, which is used to load environment variables from a .env file into the environment.\n",
    "\n",
    "4. `import os`: Imports the `os` module, which provides a way to interact with the operating system, including accessing environment variables.\n",
    "\n",
    "5. `import logging`: Imports the `logging` module, which allows for logging messages during the execution of the code.\n",
    "\n",
    "6. `from sqlalchemy.exc import SQLAlchemyError`: Imports the `SQLAlchemyError` exception class from SQLAlchemy, which is used to handle errors related to database operations.\n",
    "\n",
    "7. `import pyarrow.parquet as pq`: Imports the `pyarrow.parquet` module as `pq`, which provides functions for working with Parquet files.\n",
    "\n",
    "8. `import pyarrow as pa`: Imports the `pyarrow` library and aliases it as `pa`, which is used for data interchange and serialization.\n",
    "\n",
    "9. `load_dotenv(override=True, encoding='utf-16')`: Calls the `load_dotenv` function to load environment variables from a .env file. The `override=True` parameter indicates that existing environment variables should be overridden, and `encoding='utf-16'` specifies the encoding to use when reading the .env file.\n",
    "\n",
    "10. `database_url = os.getenv('DATABASE_URL')`: Retrieves the value of the 'DATABASE_URL' environment variable using `os.getenv` and assigns it to the `database_url` variable. This variable presumably contains the URL for connecting to a MySQL database.\n",
    "\n",
    "11. `def load_csv_to_mysql_parquet(file_path, table_name, database_url):`: Defines a function named `load_csv_to_mysql_parquet` that takes three parameters: `file_path` (path to the CSV file), `table_name` (name of the MySQL table to load the data into), and `database_url` (URL of the MySQL database).\n",
    "\n",
    "12. `try:`: Begins a try-except block to handle potential exceptions that may occur during the execution of the code.\n",
    "\n",
    "13. `engine = create_engine(database_url, pool_size=10, max_overflow=20)`: Creates a SQLAlchemy engine using the `create_engine` function, specifying the `database_url` as the connection URL, `pool_size=10` as the maximum number of connections in the connection pool, and `max_overflow=20` as the maximum number of connections that can be created beyond the `pool_size` limit.\n",
    "\n",
    "14. `df = pd.read_csv(file_path)`: Uses pandas (`pd`) to read the CSV file specified by `file_path` into a pandas DataFrame (`df`).\n",
    "\n",
    "15. `table = pa.Table.from_pandas(df)`: Converts the pandas DataFrame `df` into a PyArrow Table using `pa.Table.from_pandas`.\n",
    "\n",
    "16. `pq.write_table(table, 'temp.parquet')`: Writes the PyArrow Table `table` to a Parquet file named 'temp.parquet' using `pq.write_table`.\n",
    "\n",
    "17. `df_new = pd.read_parquet('temp.parquet')`: Reads the Parquet file 'temp.parquet' into a new pandas DataFrame `df_new` using `pd.read_parquet`.\n",
    "\n",
    "18. `df_new.to_sql(table_name, con=engine, if_exists='append', chunksize=100000, index=False)`: Uses the `to_sql` function of the pandas DataFrame `df_new` to insert data into the MySQL table specified by `table_name`. The `con=engine` parameter specifies the SQLAlchemy engine for the connection, `if_exists='append'` appends the data if the table already exists, `chunksize=100000` specifies the number of rows to insert per batch, and `index=False` indicates not to include the DataFrame index as a column in the MySQL table.\n",
    "\n",
    "19. `print(f\"Data loaded successfully into the '{table_name}' table.\" )`: Prints a success message indicating that the data was loaded successfully into the specified MySQL table.\n",
    "\n",
    "20. Various `except` blocks: These blocks handle specific exceptions that may occur during the execution of the code. They include `FileNotFoundError` for handling cases where the CSV file is not found, `pd.errors.EmptyDataError` for handling empty CSV files, `SQLAlchemyError` for handling SQLAlchemy-related errors, and a generic `Exception` block for handling any other unexpected errors. These blocks log appropriate error messages using the `logging` module.\n",
    "\n",
    "21. `finally:`: Begins a block of code that will always be executed, regardless of whether an exception occurs or not.\n",
    "\n",
    "22. `if 'engine' in locals(): engine.dispose()`: Checks if the `engine` variable is defined in the local scope (within the function) and disposes of the engine connection using `engine.dispose()` to release any resources held by the engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d716659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mysqlclient in c:\\users\\bookie\\anaconda3\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\bookie\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: pymysql in c:\\users\\bookie\\anaconda3\\lib\\site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install mysqlclient\n",
    "!pip install python-dotenv\n",
    "!pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a808ce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import logging\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "# Access environment variables\n",
    "load_dotenv(override=True, encoding='utf-16')\n",
    "\n",
    "# Access environment variables\n",
    "database_url = os.getenv('DATABASE_URL')\n",
    "\n",
    "def load_csv_to_mysql_parquet(file_path, table_name, database_url):\n",
    "    try: \n",
    "        # Create a connection to the MySQL database using the provided database URL\n",
    "        engine = create_engine(database_url, pool_size=10, max_overflow=20)\n",
    "\n",
    "        # Read the data from the csv file into a pandas dataframe\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Convert the pandas dataframe to a PyArrow Table\n",
    "        table = pa.Table.from_pandas(df)\n",
    "\n",
    "        # Write the PyArrow Table to a Parquet file\n",
    "        pq.write_table(table, 'temp.parquet')\n",
    "\n",
    "        # Read the Parquet file into a new pandas dataframe\n",
    "        df_new = pd.read_parquet('temp.parquet')\n",
    "\n",
    "        # Use pandas to_sql function to load data from DataFrame into MySQL table\n",
    "        df_new.to_sql(table_name, con=engine, if_exists='append', chunksize=100000, index=False)\n",
    "\n",
    "        print(f\"Data loaded successfully into the '{table_name}' table.\" )\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"Error: CSV file '{file_path}' not found. {e}\")\n",
    "    \n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        logging.error(f\"Error: CSV file '{file_path}' is empty. {e}\")\n",
    "        \n",
    "    except SQLAlchemyError as e:\n",
    "        logging.error(f\"SQLAlchemy Error: {e}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        if 'engine' in locals():\n",
    "            engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed5d5c9",
   "metadata": {},
   "source": [
    "##### In this modified version of your function, pd.read_csv(file_path) returns a pandas DataFrame. The DataFrame is then converted to a PyArrow Table and written to a Parquet file. The Parquet file is read back into a new pandas DataFrame, which is then written to the MySQL table using the to_sql method.\n",
    "\n",
    "##### Please note that this approach involves writing and reading a temporary Parquet file (‘temp.parquet’), so make sure you have enough disk space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16988a98",
   "metadata": {},
   "source": [
    "#### Load users data into the Users Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f09f6fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully into the 'users' table.\n"
     ]
    }
   ],
   "source": [
    "#Usage: Calling the function\n",
    "file_path = \"C:/Users/Bookie/Documents/Dufuna_Documentation/CapstoneProject/data_tables/users.csv\"\n",
    "table_name = \"users\"\n",
    "database_url = os.getenv('DATABASE_URL')\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    load_csv_to_mysql_parquet(file_path, table_name, database_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54ade08",
   "metadata": {},
   "source": [
    "**Load users data into the Events Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e102e732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully into the 'events' table.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"C:/Users/Bookie/Documents/Dufuna_Documentation/CapstoneProject/data_tables/events.csv\"\n",
    "table_name = \"events\"\n",
    "database_url = os.getenv('DATABASE_URL')\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "        load_csv_to_mysql_parquet(file_path, table_name, database_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94118bd1",
   "metadata": {},
   "source": [
    "**Load users data into the Distribution_Centers Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e6440ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully into the 'distribution_centers' table.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"C:/Users/Bookie/Documents/Dufuna_Documentation/CapstoneProject/data_tables/distribution_centers.csv\"\n",
    "table_name = \"distribution_centers\"\n",
    "database_url = os.getenv('DATABASE_URL')\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    load_csv_to_mysql_parquet(file_path, table_name, database_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c62bf8",
   "metadata": {},
   "source": [
    "**Load users data into the Orders Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6d99b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully into the 'orders' table.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"C:/Users/Bookie/Documents/Dufuna_Documentation/CapstoneProject/data_tables/orders.csv\"\n",
    "table_name = \"orders\"\n",
    "database_url = os.getenv('DATABASE_URL')\n",
    "\n",
    "if __name__==\"__main__\":    \n",
    "    load_csv_to_mysql_parquet(file_path, table_name, database_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb0f64d",
   "metadata": {},
   "source": [
    "**Load users data into the Products Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "000662aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully into the 'products' table.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"C:/Users/Bookie/Documents/Dufuna_Documentation/CapstoneProject/data_tables/products.csv\"\n",
    "table_name = \"products\"\n",
    "database_url = os.getenv('DATABASE_URL')\n",
    "\n",
    "if __name__==\"__main__\":    \n",
    "    load_csv_to_mysql_parquet(file_path, table_name, database_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad6a959",
   "metadata": {},
   "source": [
    "**Load users data into the Inventory_Items Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "278b871b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully into the 'inventory_items' table.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"C:/Users/Bookie/Documents/Dufuna_Documentation/CapstoneProject/data_tables/inventory_items.csv\"\n",
    "table_name = \"inventory_items\"\n",
    "database_url = os.getenv('DATABASE_URL')\n",
    "\n",
    "if __name__==\"__main__\":    \n",
    "    load_csv_to_mysql_parquet(file_path, table_name, database_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4b49a",
   "metadata": {},
   "source": [
    "**Load users data into the Order_Items Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ec7fd595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully into the 'order_items' table.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"C:/Users/Bookie/Documents/Dufuna_Documentation/CapstoneProject/data_tables/order_items.csv\"\n",
    "table_name = \"order_items\"\n",
    "database_url = os.getenv('DATABASE_URL')\n",
    "\n",
    "if __name__==\"__main__\":    \n",
    "    load_csv_to_mysql_parquet(file_path, table_name, database_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ece92b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
